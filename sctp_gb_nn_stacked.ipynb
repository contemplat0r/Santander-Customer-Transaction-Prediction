{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import logging\n",
    "import time\n",
    "import math\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from scipy.stats.kde import gaussian_kde\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, roc_curve, accuracy_score, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "#from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, KFold\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "#from sklearn import cluster\n",
    "\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization, Input, Conv2D\n",
    "from keras import callbacks\n",
    "from keras import metrics as keras_metrics\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras import regularizers\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred): \n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.lr = []\n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.lr.append(step_decay(len(self.losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simply_blend(prediction_dataframes, weights, target_column_name='target'):\n",
    "    blended_prediction_df = pd.DataFrame(data=np.zeros(prediction_dataframes[0].shape[0]), columns=[target_column_name])\n",
    "    for prediction_df, weight in zip(prediction_dataframes, weights):\n",
    "        blended_prediction_df[target_column_name] = blended_prediction_df[target_column_name] + weight * prediction_df[target_column_name]\n",
    "        #blended_prediction_df.add(weight * prediction_df)\n",
    "    return blended_prediction_df / sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_keras_nn(train_features, train_targets, train_meta_df, train_meta_column_name, model, batch_size=100, epochs=20, n_splits=5):\n",
    "    loss_history = LossHistory()\n",
    "    lrate = LearningRateScheduler(step_decay)\n",
    "    callbacks_list = [EarlyStopping(monitor='val_auc', patience=20, mode='max'), loss_history, annealer]\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits)\n",
    "    start_time = time.time()\n",
    "    print(type(train_features))\n",
    "    for train_index, val_index in sss.split(train_features, train_targets):\n",
    "        #print(type(train_index))\n",
    "        #print(train_index)\n",
    "        X_train = train_features[train_index]\n",
    "        X_val = train_features[val_index]\n",
    "        Y_train = train_targets[train_index]\n",
    "        Y_val = train_targets[val_index]\n",
    "        #X_tr, Y_tr = augment(X_train, Y_train)\n",
    "        #print(\"{} iteration\".format(i+1))\n",
    "        history= model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, callbacks=callbacks_list, verbose=1, validation_data=(X_val,Y_val))\n",
    "        predictions = model.predict(X_val)[:, 0]\n",
    "        print(\"val_index.shape: \", val_index.shape)\n",
    "        print(\"predictions.shape: \", predictions.shape)\n",
    "        train_meta_df[train_meta_column_name].iloc[val_index] = predictions\n",
    "        #history= sequential_nn_model.fit(X_train, Y_train, batch_size=batch_size, epochs=50, callbacks=callbacks_list, verbose=1, validation_data=(X_val,Y_val))\n",
    "        del X_train, X_val, Y_train, Y_val\n",
    "        gc.collect()\n",
    "    print(\"Run time {} min\".format((time.time() - start_time) / 60))\n",
    "    return model, train_meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(train_df, test_df, target, features, param, num_round=1000000):\n",
    "    start_time = time.time()\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=False, random_state=44000)\n",
    "    oof = np.zeros(len(train_df))\n",
    "    predictions = np.zeros(len(test_df))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    lgb_classifier = None\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n",
    "        print(\"Fold {}\".format(fold_))\n",
    "        trn_data = lgb.Dataset(train_df.iloc[trn_idx][features], label=target.iloc[trn_idx])\n",
    "        val_data = lgb.Dataset(train_df.iloc[val_idx][features], label=target.iloc[val_idx])\n",
    "        num_round = num_round\n",
    "        clf = lgb.train(\n",
    "            param,\n",
    "            trn_data,\n",
    "            num_round,\n",
    "            valid_sets=[trn_data, val_data],\n",
    "            verbose_eval=1000,\n",
    "            early_stopping_rounds=3000\n",
    "        )\n",
    "        lgb_classifier = clf\n",
    "        oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['Feature'] = features\n",
    "        fold_importance_df['importance'] = clf.feature_importance()\n",
    "        fold_importance_df['fold'] = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        predictions += clf.predict(test_df[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "    print(\"Total run time {} min:\".format((time.time() - start_time) / 60))\n",
    "    print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))\n",
    "    return oof, predictions, feature_importance_df, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pdf_difference(feat, df_feature, df_target, IQR_multiplier, bin_bandwidth_multiplier, print_number_bins):\n",
    "    #Agreggating feature values in bin format using the Freedman-Diaconis rule\n",
    "    IQR = df_feature[feat].quantile([0.75]).values - df_feature[feat].quantile([0.25]).values #Interquartile range (IQR)\n",
    "    n = len(df_feature[feat])\n",
    "    bin_size = IQR_multiplier*IQR/n**(1/3)\n",
    "    bin_number = int(np.round((df_feature[feat].max() - df_feature[feat].min())/bin_size))\n",
    "    binvalues = pd.cut(df_feature[feat], bins = bin_number, labels = range(bin_number)).astype('float')\n",
    "    \n",
    "    if print_number_bins:\n",
    "        print('There are {} bins in the feature {}'.format(bin_number, feat))\n",
    "\n",
    "    #Calculate the PDFs using the df_target\n",
    "    pdf_0 = KernelDensity(kernel='gaussian', bandwidth=bin_size*bin_bandwidth_multiplier)\n",
    "    pdf_0.fit(np.array(df_target[feat][df_target['target'] == 0]).reshape(-1,1))\n",
    "\n",
    "    pdf_1 = KernelDensity(kernel='gaussian', bandwidth=bin_size*bin_bandwidth_multiplier)\n",
    "    pdf_1.fit(np.array(df_target[feat][df_target['target'] == 1]).reshape(-1,1))\n",
    "\n",
    "    #Creates an X array with the average feature value for each bin\n",
    "    x = np.array(np.arange(min(df_feature[feat]) + bin_size/2 ,max(df_feature[feat]), bin_size)).reshape(-1,1)\n",
    "\n",
    "    #gets the pdf values based on the X array\n",
    "    log_pdf_0 = np.exp(pdf_0.score_samples(x))\n",
    "    log_pdf_1 = np.exp(pdf_1.score_samples(x))\n",
    "\n",
    "    #creates a dictionary that links the bin number with the PDFs value difference\n",
    "    pdf_dict = dict()\n",
    "    for i in range(bin_number):\n",
    "        pdf_dict[i] = log_pdf_1[i] - log_pdf_0[i] \n",
    "\n",
    "    #gets the PDF difference for each row of the dataset based on its equivalent bin.\n",
    "    bin_pdf_values = np.array(itemgetter(*list(binvalues))(pdf_dict))\n",
    "\n",
    "    return bin_pdf_values, x, log_pdf_0, log_pdf_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = [c for c in train_df.columns if c not in ['ID_code', 'target']]\n",
    "train_features_df = train_df[train_df.columns.drop(['ID_code', 'target'])].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_series = train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id_codes = train_df['ID_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values, holdout_test_values, train_target_values, holdout_test_target_values = train_test_split(\n",
    "    train_features_df.values,\n",
    "    target_series.values,\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values_df, holdout_test_df, train_target_series, holdout_test_target_series = train_test_split(\n",
    "    train_features_df,\n",
    "    target_series,\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train_values.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_regularizer=regularizers.l2(0.01)\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(input_dim,)))\n",
    "#model.add(PreLU(alpha=.001))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64, input_shape=(input_dim / 2, ), activation='relu'))\n",
    "#model.add(PreLU(alpha=.001))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32, input_shape=(input_dim / 4, ), activation='relu'))\n",
    "#model.add(PreLU(alpha=.001))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "annealer = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_nn_model = Sequential()\n",
    "sequential_nn_model.add(Dense(batch_size, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "sequential_nn_model.add(Dropout(0.1))\n",
    "#sequential_nn_model.add(Dropout(0.4))\n",
    "sequential_nn_model.add(BatchNormalization())\n",
    "sequential_nn_model.add(Dense(batch_size, input_dim=input_dim / 2, kernel_initializer='normal', activation='sigmoid'))\n",
    "####sequential_nn_model.add(Dense(batch_size, input_dim=input_dim / 10, kernel_initializer='normal', activation='sigmoid'))\n",
    "sequential_nn_model.add(Dropout(0.1))\n",
    "####sequential_nn_model.add(Dropout(0.4))\n",
    "sequential_nn_model.add(BatchNormalization())\n",
    "sequential_nn_model.add(Dense(batch_size, input_dim=input_dim / 4, kernel_initializer='normal', activation='relu'))\n",
    "sequential_nn_model.add(Dropout(0.1))\n",
    "sequential_nn_model.add(BatchNormalization())\n",
    "sequential_nn_model.add(Dense(batch_size, input_dim=input_dim / 4, kernel_initializer='normal', activation='sigmoid'))\n",
    "#sequential_nn_model.add(Dense(batch_size, input_shape=(100, 200), kernel_initializer='normal', activation='sigmoid'))\n",
    "#sequential_nn_model.add(Dropout(0.76))\n",
    "#sequential_nn_model.add(Dropout(0.24))\n",
    "sequential_nn_model.add(Dropout(0.1))\n",
    "sequential_nn_model.add(BatchNormalization())\n",
    "sequential_nn_model.add(Dense(batch_size, input_dim=input_dim / 30, kernel_initializer='normal', activation='relu'))\n",
    "sequential_nn_model.add(Dropout(0.1))\n",
    "sequential_nn_model.add(BatchNormalization())\n",
    "sequential_nn_model.add(Dense(batch_size, input_dim=input_dim / 30, kernel_initializer='normal', activation='sigmoid'))\n",
    "sequential_nn_model.add(Dropout(0.1))\n",
    "sequential_nn_model.add(BatchNormalization())\n",
    "sequential_nn_model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequential_model_min(input_dim):\n",
    "    sequential_nn_model_min = Sequential()\n",
    "    sequential_nn_model_min.add(Dense(batch_size, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "    sequential_nn_model_min.add(Dropout(0.4))\n",
    "    sequential_nn_model_min.add(BatchNormalization())\n",
    "    sequential_nn_model_min.add(Dense(batch_size, input_dim=input_dim / 10, kernel_initializer='normal', activation='sigmoid'))\n",
    "    sequential_nn_model_min.add(Dropout(0.4))\n",
    "    sequential_nn_model_min.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    return sequential_nn_model_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_nn_model_min = make_sequential_model_min(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_nn_model_min.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_df = train_features_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_df['nn_meta'] = np.NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_df['bayes_meta'] = np.NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_meta_df['nn_meta'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction = pd.Series([0.2, 0.22, 0.3, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_meta_df['nn_meta'].iloc[[2, 3, 4, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_meta_df['nn_meta'].iloc[[2, 3, 4, 5]] = prediction.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_meta_df['nn_meta'].iloc[[2, 3, 4, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_meta_df['nn_meta'].iloc[[0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/20\n",
      "144000/144000 [==============================] - 4s 28us/step - loss: 0.2830 - acc: 0.9000 - auc: 0.7719 - val_loss: 0.2584 - val_acc: 0.9084 - val_auc: 0.8273\n",
      "Epoch 2/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2715 - acc: 0.9028 - auc: 0.7951 - val_loss: 0.2946 - val_acc: 0.8967 - val_auc: 0.8290\n",
      "Epoch 3/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2697 - acc: 0.9046 - auc: 0.7977 - val_loss: 0.2600 - val_acc: 0.9007 - val_auc: 0.8354\n",
      "Epoch 4/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2650 - acc: 0.9047 - auc: 0.8075 - val_loss: 0.2528 - val_acc: 0.9012 - val_auc: 0.8355\n",
      "Epoch 5/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2639 - acc: 0.9031 - auc: 0.8115 - val_loss: 0.2648 - val_acc: 0.8995 - val_auc: 0.8373\n",
      "Epoch 6/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2685 - acc: 0.8995 - auc: 0.8068 - val_loss: 0.2535 - val_acc: 0.8995 - val_auc: 0.8373\n",
      "Epoch 7/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2628 - acc: 0.8993 - auc: 0.8178 - val_loss: 0.2610 - val_acc: 0.8995 - val_auc: 0.8414\n",
      "Epoch 8/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2601 - acc: 0.8995 - auc: 0.8216 - val_loss: 0.2633 - val_acc: 0.8995 - val_auc: 0.8419\n",
      "Epoch 9/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2579 - acc: 0.9000 - auc: 0.8256 - val_loss: 0.2598 - val_acc: 0.8995 - val_auc: 0.8419\n",
      "Epoch 10/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2568 - acc: 0.8994 - auc: 0.8280 - val_loss: 0.2626 - val_acc: 0.9125 - val_auc: 0.8435\n",
      "Epoch 11/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2560 - acc: 0.8992 - auc: 0.8300 - val_loss: 0.2597 - val_acc: 0.8995 - val_auc: 0.8446\n",
      "Epoch 12/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2542 - acc: 0.9002 - auc: 0.8315 - val_loss: 0.2653 - val_acc: 0.9083 - val_auc: 0.8438\n",
      "Epoch 13/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2535 - acc: 0.9007 - auc: 0.8333 - val_loss: 0.2648 - val_acc: 0.8995 - val_auc: 0.8451\n",
      "Epoch 14/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2540 - acc: 0.9003 - auc: 0.8330 - val_loss: 0.2702 - val_acc: 0.9133 - val_auc: 0.8458\n",
      "Epoch 15/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2524 - acc: 0.9010 - auc: 0.8355 - val_loss: 0.2729 - val_acc: 0.8995 - val_auc: 0.8457\n",
      "Epoch 16/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2526 - acc: 0.9003 - auc: 0.8349 - val_loss: 0.2719 - val_acc: 0.9131 - val_auc: 0.8458\n",
      "Epoch 17/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2521 - acc: 0.9004 - auc: 0.8354 - val_loss: 0.2627 - val_acc: 0.9122 - val_auc: 0.8458\n",
      "Epoch 18/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2515 - acc: 0.9013 - auc: 0.8357 - val_loss: 0.2517 - val_acc: 0.8995 - val_auc: 0.8460\n",
      "Epoch 19/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2517 - acc: 0.9006 - auc: 0.8363 - val_loss: 0.2626 - val_acc: 0.9114 - val_auc: 0.8475\n",
      "Epoch 20/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2517 - acc: 0.9008 - auc: 0.8364 - val_loss: 0.2594 - val_acc: 0.8995 - val_auc: 0.8464\n",
      "val_index.shape:  (16000,)\n",
      "predictions.shape:  (16000,)\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2547 - acc: 0.9001 - auc: 0.8330 - val_loss: 0.3075 - val_acc: 0.9064 - val_auc: 0.8485\n",
      "Epoch 2/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2543 - acc: 0.9002 - auc: 0.8339 - val_loss: 0.2644 - val_acc: 0.9086 - val_auc: 0.8492\n",
      "Epoch 3/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2536 - acc: 0.9003 - auc: 0.8348 - val_loss: 0.2639 - val_acc: 0.8995 - val_auc: 0.8512\n",
      "Epoch 4/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2523 - acc: 0.9000 - auc: 0.8364 - val_loss: 0.2747 - val_acc: 0.9119 - val_auc: 0.8515\n",
      "Epoch 5/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2527 - acc: 0.9004 - auc: 0.8355 - val_loss: 0.2746 - val_acc: 0.8995 - val_auc: 0.8504\n",
      "Epoch 6/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2522 - acc: 0.9005 - auc: 0.8361 - val_loss: 0.2666 - val_acc: 0.9127 - val_auc: 0.8513\n",
      "Epoch 7/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2520 - acc: 0.9008 - auc: 0.8368 - val_loss: 0.2658 - val_acc: 0.8995 - val_auc: 0.8514\n",
      "Epoch 8/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2515 - acc: 0.9003 - auc: 0.8367 - val_loss: 0.2534 - val_acc: 0.9099 - val_auc: 0.8508\n",
      "Epoch 9/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2518 - acc: 0.9008 - auc: 0.8361 - val_loss: 0.2726 - val_acc: 0.9126 - val_auc: 0.8522\n",
      "Epoch 10/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2510 - acc: 0.9013 - auc: 0.8378 - val_loss: 0.2653 - val_acc: 0.8995 - val_auc: 0.8520\n",
      "Epoch 11/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2510 - acc: 0.9007 - auc: 0.8382 - val_loss: 0.2621 - val_acc: 0.9131 - val_auc: 0.8522\n",
      "Epoch 12/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2501 - acc: 0.9014 - auc: 0.8395 - val_loss: 0.2548 - val_acc: 0.9081 - val_auc: 0.8523\n",
      "Epoch 13/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2515 - acc: 0.9003 - auc: 0.8372 - val_loss: 0.2741 - val_acc: 0.8995 - val_auc: 0.8524\n",
      "Epoch 14/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2499 - acc: 0.9009 - auc: 0.8395 - val_loss: 0.2684 - val_acc: 0.9107 - val_auc: 0.8530\n",
      "Epoch 15/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2495 - acc: 0.9017 - auc: 0.8394 - val_loss: 0.2587 - val_acc: 0.9119 - val_auc: 0.8530\n",
      "Epoch 16/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2496 - acc: 0.9009 - auc: 0.8402 - val_loss: 0.2617 - val_acc: 0.9086 - val_auc: 0.8535\n",
      "Epoch 17/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2499 - acc: 0.9019 - auc: 0.8389 - val_loss: 0.2637 - val_acc: 0.9075 - val_auc: 0.8536\n",
      "Epoch 18/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2488 - acc: 0.9017 - auc: 0.8413 - val_loss: 0.2703 - val_acc: 0.9113 - val_auc: 0.8534\n",
      "Epoch 19/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2492 - acc: 0.9014 - auc: 0.8407 - val_loss: 0.2612 - val_acc: 0.9117 - val_auc: 0.8531\n",
      "Epoch 20/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2494 - acc: 0.9010 - auc: 0.8402 - val_loss: 0.2538 - val_acc: 0.9103 - val_auc: 0.8536\n",
      "val_index.shape:  (16000,)\n",
      "predictions.shape:  (16000,)\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2517 - acc: 0.9008 - auc: 0.8374 - val_loss: 0.2488 - val_acc: 0.9086 - val_auc: 0.8589\n",
      "Epoch 2/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2520 - acc: 0.9005 - auc: 0.8372 - val_loss: 0.2734 - val_acc: 0.9113 - val_auc: 0.8566\n",
      "Epoch 3/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2520 - acc: 0.9002 - auc: 0.8368 - val_loss: 0.2465 - val_acc: 0.8995 - val_auc: 0.8572\n",
      "Epoch 4/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2518 - acc: 0.9005 - auc: 0.8371 - val_loss: 0.2530 - val_acc: 0.9054 - val_auc: 0.8576\n",
      "Epoch 5/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2513 - acc: 0.9004 - auc: 0.8384 - val_loss: 0.2525 - val_acc: 0.8995 - val_auc: 0.8572\n",
      "Epoch 6/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2516 - acc: 0.9003 - auc: 0.8380 - val_loss: 0.2745 - val_acc: 0.9116 - val_auc: 0.8564\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2510 - acc: 0.9007 - auc: 0.8384 - val_loss: 0.2541 - val_acc: 0.9136 - val_auc: 0.8579\n",
      "Epoch 8/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2502 - acc: 0.9012 - auc: 0.8391 - val_loss: 0.2648 - val_acc: 0.8995 - val_auc: 0.8576\n",
      "Epoch 9/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2502 - acc: 0.9010 - auc: 0.8394 - val_loss: 0.2845 - val_acc: 0.9137 - val_auc: 0.8577\n",
      "Epoch 10/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2498 - acc: 0.9010 - auc: 0.8392 - val_loss: 0.2534 - val_acc: 0.8995 - val_auc: 0.8580\n",
      "Epoch 11/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2501 - acc: 0.9010 - auc: 0.8399 - val_loss: 0.2681 - val_acc: 0.9114 - val_auc: 0.8580\n",
      "Epoch 12/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2497 - acc: 0.9017 - auc: 0.8403 - val_loss: 0.2743 - val_acc: 0.9138 - val_auc: 0.8577\n",
      "Epoch 13/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2495 - acc: 0.9016 - auc: 0.8406 - val_loss: 0.2552 - val_acc: 0.9144 - val_auc: 0.8581\n",
      "Epoch 14/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2493 - acc: 0.9007 - auc: 0.8405 - val_loss: 0.2613 - val_acc: 0.8995 - val_auc: 0.8576\n",
      "Epoch 15/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2493 - acc: 0.9015 - auc: 0.8412 - val_loss: 0.2632 - val_acc: 0.8995 - val_auc: 0.8578\n",
      "Epoch 16/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2492 - acc: 0.9015 - auc: 0.8405 - val_loss: 0.2773 - val_acc: 0.9146 - val_auc: 0.8585\n",
      "Epoch 17/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2492 - acc: 0.9016 - auc: 0.8410 - val_loss: 0.2662 - val_acc: 0.9124 - val_auc: 0.8590\n",
      "Epoch 18/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2486 - acc: 0.9015 - auc: 0.8415 - val_loss: 0.2574 - val_acc: 0.9125 - val_auc: 0.8579\n",
      "Epoch 19/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2482 - acc: 0.9017 - auc: 0.8418 - val_loss: 0.2648 - val_acc: 0.9145 - val_auc: 0.8581\n",
      "Epoch 20/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2483 - acc: 0.9019 - auc: 0.8426 - val_loss: 0.2535 - val_acc: 0.9107 - val_auc: 0.8591\n",
      "val_index.shape:  (16000,)\n",
      "predictions.shape:  (16000,)\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2506 - acc: 0.9008 - auc: 0.8396 - val_loss: 0.2642 - val_acc: 0.8995 - val_auc: 0.8503\n",
      "Epoch 2/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2507 - acc: 0.9009 - auc: 0.8393 - val_loss: 0.2605 - val_acc: 0.8995 - val_auc: 0.8511\n",
      "Epoch 3/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2508 - acc: 0.9005 - auc: 0.8395 - val_loss: 0.2643 - val_acc: 0.9099 - val_auc: 0.8493\n",
      "Epoch 4/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2496 - acc: 0.9013 - auc: 0.8409 - val_loss: 0.2829 - val_acc: 0.9138 - val_auc: 0.8485\n",
      "Epoch 5/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2498 - acc: 0.9011 - auc: 0.8402 - val_loss: 0.2664 - val_acc: 0.9105 - val_auc: 0.8502\n",
      "Epoch 6/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2492 - acc: 0.9015 - auc: 0.8407 - val_loss: 0.2744 - val_acc: 0.9127 - val_auc: 0.8481\n",
      "Epoch 7/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2498 - acc: 0.9004 - auc: 0.8406 - val_loss: 0.2698 - val_acc: 0.9134 - val_auc: 0.8488\n",
      "Epoch 8/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2495 - acc: 0.9007 - auc: 0.8411 - val_loss: 0.2714 - val_acc: 0.9132 - val_auc: 0.8495\n",
      "Epoch 9/20\n",
      "144000/144000 [==============================] - 2s 14us/step - loss: 0.2481 - acc: 0.9018 - auc: 0.8417 - val_loss: 0.2713 - val_acc: 0.8995 - val_auc: 0.8494\n",
      "Epoch 10/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2486 - acc: 0.9014 - auc: 0.8415 - val_loss: 0.2801 - val_acc: 0.9126 - val_auc: 0.8493\n",
      "Epoch 11/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2479 - acc: 0.9026 - auc: 0.8427 - val_loss: 0.2726 - val_acc: 0.9122 - val_auc: 0.8503\n",
      "Epoch 12/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2487 - acc: 0.9013 - auc: 0.8421 - val_loss: 0.2737 - val_acc: 0.9134 - val_auc: 0.8500\n",
      "Epoch 13/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2477 - acc: 0.9017 - auc: 0.8430 - val_loss: 0.2709 - val_acc: 0.9133 - val_auc: 0.8499\n",
      "Epoch 14/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2484 - acc: 0.9015 - auc: 0.8423 - val_loss: 0.2704 - val_acc: 0.9141 - val_auc: 0.8502\n",
      "Epoch 15/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2479 - acc: 0.9018 - auc: 0.8432 - val_loss: 0.2740 - val_acc: 0.9129 - val_auc: 0.8509\n",
      "Epoch 16/20\n",
      "144000/144000 [==============================] - 2s 14us/step - loss: 0.2477 - acc: 0.9019 - auc: 0.8433 - val_loss: 0.2645 - val_acc: 0.9129 - val_auc: 0.8508\n",
      "Epoch 17/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2475 - acc: 0.9013 - auc: 0.8434 - val_loss: 0.2717 - val_acc: 0.9133 - val_auc: 0.8504\n",
      "Epoch 18/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2474 - acc: 0.9027 - auc: 0.8431 - val_loss: 0.2613 - val_acc: 0.9128 - val_auc: 0.8508\n",
      "Epoch 19/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2480 - acc: 0.9023 - auc: 0.8428 - val_loss: 0.2638 - val_acc: 0.9092 - val_auc: 0.8503\n",
      "Epoch 20/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2470 - acc: 0.9023 - auc: 0.8441 - val_loss: 0.2684 - val_acc: 0.9133 - val_auc: 0.8504\n",
      "val_index.shape:  (16000,)\n",
      "predictions.shape:  (16000,)\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2508 - acc: 0.9016 - auc: 0.8394 - val_loss: 0.2523 - val_acc: 0.8995 - val_auc: 0.8559\n",
      "Epoch 2/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2512 - acc: 0.9007 - auc: 0.8380 - val_loss: 0.2707 - val_acc: 0.8995 - val_auc: 0.8574\n",
      "Epoch 3/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2506 - acc: 0.9013 - auc: 0.8391 - val_loss: 0.2545 - val_acc: 0.9073 - val_auc: 0.8573\n",
      "Epoch 4/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2490 - acc: 0.9021 - auc: 0.8411 - val_loss: 0.2639 - val_acc: 0.8995 - val_auc: 0.8566\n",
      "Epoch 5/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2496 - acc: 0.9013 - auc: 0.8402 - val_loss: 0.2595 - val_acc: 0.9091 - val_auc: 0.8569\n",
      "Epoch 6/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2492 - acc: 0.9012 - auc: 0.8412 - val_loss: 0.2724 - val_acc: 0.9121 - val_auc: 0.8567\n",
      "Epoch 7/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2491 - acc: 0.9010 - auc: 0.8411 - val_loss: 0.2692 - val_acc: 0.9114 - val_auc: 0.8572\n",
      "Epoch 8/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2489 - acc: 0.9014 - auc: 0.8411 - val_loss: 0.2573 - val_acc: 0.9093 - val_auc: 0.8561\n",
      "Epoch 9/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2491 - acc: 0.9011 - auc: 0.8413 - val_loss: 0.2838 - val_acc: 0.8995 - val_auc: 0.8574\n",
      "Epoch 10/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2485 - acc: 0.9017 - auc: 0.8414 - val_loss: 0.2575 - val_acc: 0.8995 - val_auc: 0.8573\n",
      "Epoch 11/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2491 - acc: 0.9013 - auc: 0.8410 - val_loss: 0.2608 - val_acc: 0.9107 - val_auc: 0.8566\n",
      "Epoch 12/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2483 - acc: 0.9013 - auc: 0.8425 - val_loss: 0.2714 - val_acc: 0.9110 - val_auc: 0.8567\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2480 - acc: 0.9021 - auc: 0.8428 - val_loss: 0.2617 - val_acc: 0.9114 - val_auc: 0.8576\n",
      "Epoch 14/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2480 - acc: 0.9017 - auc: 0.8429 - val_loss: 0.2622 - val_acc: 0.9110 - val_auc: 0.8571\n",
      "Epoch 15/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2477 - acc: 0.9019 - auc: 0.8432 - val_loss: 0.2773 - val_acc: 0.9119 - val_auc: 0.8572\n",
      "Epoch 16/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2475 - acc: 0.9025 - auc: 0.8433 - val_loss: 0.2604 - val_acc: 0.9119 - val_auc: 0.8575\n",
      "Epoch 17/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2474 - acc: 0.9022 - auc: 0.8435 - val_loss: 0.2660 - val_acc: 0.9121 - val_auc: 0.8577\n",
      "Epoch 18/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2473 - acc: 0.9020 - auc: 0.8436 - val_loss: 0.2640 - val_acc: 0.9123 - val_auc: 0.8574\n",
      "Epoch 19/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2466 - acc: 0.9023 - auc: 0.8446 - val_loss: 0.2717 - val_acc: 0.9119 - val_auc: 0.8577\n",
      "Epoch 20/20\n",
      "144000/144000 [==============================] - 2s 13us/step - loss: 0.2465 - acc: 0.9034 - auc: 0.8443 - val_loss: 0.2791 - val_acc: 0.9111 - val_auc: 0.8575\n",
      "val_index.shape:  (16000,)\n",
      "predictions.shape:  (16000,)\n",
      "Run time 3.289361608028412 min\n"
     ]
    }
   ],
   "source": [
    "#train_values, holdout_test_values, train_target_values, holdout_test_target_values\n",
    "train_nn_result = train_keras_nn(train_values, train_target_values, train_meta_df, 'nn_meta', sequential_nn_model_min, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_nn_model_min, train_meta_df = train_nn_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              NaN\n",
       "1              NaN\n",
       "2              NaN\n",
       "3              NaN\n",
       "4              NaN\n",
       "5         0.059299\n",
       "6         0.113107\n",
       "7         0.403068\n",
       "8              NaN\n",
       "9              NaN\n",
       "10        0.029102\n",
       "11             NaN\n",
       "12             NaN\n",
       "13        0.353431\n",
       "14        0.174848\n",
       "15             NaN\n",
       "16        0.081822\n",
       "17             NaN\n",
       "18             NaN\n",
       "19        0.279812\n",
       "20             NaN\n",
       "21             NaN\n",
       "22        0.158060\n",
       "23        0.266456\n",
       "24        0.225823\n",
       "25        0.121764\n",
       "26        0.117786\n",
       "27             NaN\n",
       "28        0.149639\n",
       "29             NaN\n",
       "            ...   \n",
       "199970         NaN\n",
       "199971         NaN\n",
       "199972         NaN\n",
       "199973         NaN\n",
       "199974         NaN\n",
       "199975         NaN\n",
       "199976         NaN\n",
       "199977         NaN\n",
       "199978         NaN\n",
       "199979         NaN\n",
       "199980         NaN\n",
       "199981         NaN\n",
       "199982         NaN\n",
       "199983         NaN\n",
       "199984         NaN\n",
       "199985         NaN\n",
       "199986         NaN\n",
       "199987         NaN\n",
       "199988         NaN\n",
       "199989         NaN\n",
       "199990         NaN\n",
       "199991         NaN\n",
       "199992         NaN\n",
       "199993         NaN\n",
       "199994         NaN\n",
       "199995         NaN\n",
       "199996         NaN\n",
       "199997         NaN\n",
       "199998         NaN\n",
       "199999         NaN\n",
       "Name: nn_meta, Length: 200000, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta_df['nn_meta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques_dict = {column_name: train_df[column_name].unique() for column_name in train_df.columns.drop(['ID_code','target']).tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques_dict_counts = {column_name: uniques.shape[0] for column_name, uniques in uniques_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques_counts_series = pd.Series(uniques_dict_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "print(uniques_counts_series.unique().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169968\n"
     ]
    }
   ],
   "source": [
    "print(uniques_counts_series.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451\n"
     ]
    }
   ],
   "source": [
    "print(uniques_counts_series.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_param = {\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.4,\n",
    "    'boost_from_average':'false',\n",
    "    'boost': 'gbdt',\n",
    "    'feature_fraction': 0.05,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': -1,  \n",
    "    'metric':'auc',\n",
    "    'min_data_in_leaf': 80,\n",
    "    'min_sum_hessian_in_leaf': 10.0,\n",
    "    'num_leaves': 13,\n",
    "    'num_threads': 8,\n",
    "    'tree_learner': 'serial',\n",
    "    'objective': 'binary', \n",
    "    'verbosity': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n",
    "        .groupby(\"Feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:150].index)\n",
    "best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,28))\n",
    "sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n",
    "plt.title('Features importance (averaged/folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('FI.png')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df_rows_count = train_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df_rows_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniques_count_more_1_2 = uniques_counts_series[uniques_counts_series > train_df_rows_count / 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniques_count_more_1_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniques_count_more_1_4_less_1_2 = uniques_counts_series[uniques_counts_series < train_df_rows_count / 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniques_count_less_1_2_more_1_4 = uniques_count_more_1_4_less_1_2[uniques_count_more_1_4_less_1_2 > train_df_rows_count / 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniques_count_more_1_4_less_1_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniques_count_less_1_4 = uniques_counts_series[uniques_counts_series < train_df_rows_count / 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniques_count_less_1_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_whole = train_lgbm(\n",
    "    train_values,\n",
    "    test_df,\n",
    "    train_target_values,\n",
    "    train_df.columns.drop(['ID_code', 'target']).tolist(),\n",
    "    lgbm_param\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oof_ucm_whole, predictions_whole, feature_importance_whole, clf_ucm_whole = train_results_whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions_df_whole = pd.DataFrame(data=predictions_whole, columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission_whole_only_df = pd.DataFrame({'ID_code': ID_code, 'target': predictions_df_whole['target'].values.astype('float32')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission_whole_only_df.to_csv('submission_whole_only.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#secquential_nn_model_min = train_keras_nn(train_polinomial_values_ucm_1_2, train_target_values_ucm_1_2, sequential_nn_model_min_1_2, batch_size=512, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_and_metrics = sequential_nn_model_min.evaluate(holdout_test_polinomial_values_ucm_1_2, holdout_test_target_values_ucm_1_2, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequential_nn_model_min.save('secquential_nn_model_min.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id_code = test_df['ID_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_submission_df = pd.read_csv('submission_mlp_not_my.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_submission_df = pd.read_csv('submission_whole_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simply_blend_gb_and_nn_df = simply_blend([mlp_submission_df, whole_submission_df], [1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\"ID_code\" : test_id_code.values, \"target\" : simply_blend_gb_and_nn_df['target']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('simply_blend_gb_and_nn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_df['log_reg_meta'] = np.NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model = LogisticRegression(C=1, n_jobs=10, penalty=\"l2\", solver='lbfgs')\n",
    "#log_reg_model = LogisticRegression(C=1, n_jobs=10, solver='newton-cg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df\n",
    "target = train_target_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "start_time = time.time()\n",
    "aucs=[]\n",
    "\n",
    "test_preds = []\n",
    "# for early stopping\n",
    "# it takes a long time if using all the samples.\n",
    "samples = train.shape[0]\n",
    "for fold,(train_idx, val_idx) in enumerate(kfold.split(train, target)):\n",
    "    print(\"####################################\")\n",
    "    print(\"############fold:\", fold)\n",
    "    sample_x = train.iloc[train_idx].values\n",
    "    sample_y = target.iloc[train_idx].values\n",
    "    \n",
    "    sample_val_x = train.iloc[val_idx].values\n",
    "    sample_val_y = target.iloc[val_idx].values\n",
    "    \n",
    "    log_reg_model.fit(sample_x,sample_y)\n",
    "    y_pred_prob = log_reg_model.predict_proba(sample_x)[:,1]\n",
    "    #y_val_pred_prob = model.predict_proba(sample_val_x)[:,1]\n",
    "    y_val_pred_prob = model.predict_proba(sample_val_x)[:, 0]\n",
    "    print(\"type(val_idx): \", type(val_idx))\n",
    "    print(\"type(y_val_pred_proba): \", type(y_val_pred_prob))\n",
    "    print(\"val_idx.shape: \", val_idx.shape)\n",
    "    print(\"y_val_pred_proba.shape: \", y_val_pred_prob.shape)\n",
    "    print(\"val_idx: \", val_idx)\n",
    "    print(\"y_val_pred_proba: \", y_val_pred_prob)    \n",
    "    train_meta_df['log_reg_meta'].iloc[val_idx] = y_val_pred_prob\n",
    "   \n",
    "    train_auc = metrics.roc_auc_score(sample_y,y_pred_prob)\n",
    "    val_auc = metrics.roc_auc_score(sample_val_y,y_val_pred_prob)\n",
    "    print(\"train auc:{},val auc:{}\".format(train_auc,val_auc))\n",
    "    aucs.append([train_auc,val_auc])\n",
    "    #test_preds.append(model.predict_proba(test)[:,1])\n",
    "    \n",
    "end_time=time.time()\n",
    "val_aucs=[auc[1] for auc in aucs]\n",
    "print(\"using {} samples,total time:{}s,mean val auc:{}\".format(samples,end_time-start_time,np.mean(val_aucs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_df['log_reg_meta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_meta_df['log_reg_meta'].isna() == False).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "feat1, feat2 = 'var_81', 'var_139'\n",
    "\n",
    "fig = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "#plot pdf feat 1\n",
    "bin_pdf_values, x, log_pdf_0, log_pdf_1 = calculate_pdf_difference(feat = feat1, df_feature = full, df_target = train_df, IQR_multiplier = 2, bin_bandwidth_multiplier = 1.5, print_number_bins = True)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "sns.kdeplot(train_df[feat1][train_df['target'] == 0], shade=False, color=\"b\", label = 'target = 0')\n",
    "sns.kdeplot(train_df[feat1][train_df['target'] == 1], shade=False, color=\"r\", label = 'target = 1')\n",
    "plt.plot(x, log_pdf_0)\n",
    "plt.plot(x, log_pdf_1) \n",
    "plt.title(feat1)\n",
    "plt.xlabel('Feature Values')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "#plot pdf feat 2\n",
    "bin_pdf_values, x, log_pdf_0, log_pdf_1 = calculate_pdf_difference(feat = feat2, df_feature = full, df_target = train_df, IQR_multiplier = 2, bin_bandwidth_multiplier = 1.5, print_number_bins = True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.kdeplot(train_df[feat2][train_df['target'] == 0], shade=False, color=\"b\", label = 'target = 0')\n",
    "sns.kdeplot(train_df[feat2][train_df['target'] == 1], shade=False, color=\"r\", label = 'target = 1')\n",
    "plt.plot(x, log_pdf_0)\n",
    "plt.plot(x, log_pdf_1) \n",
    "plt.title(feat2)\n",
    "plt.xlabel('Feature Values')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "plt.show()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
