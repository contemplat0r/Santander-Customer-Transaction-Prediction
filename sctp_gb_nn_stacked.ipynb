{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import logging\n",
    "import time\n",
    "import math\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from scipy.stats.kde import gaussian_kde\n",
    "\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, roc_curve, accuracy_score, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "#from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, KFold\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "#from sklearn import cluster\n",
    "\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization, Input, Conv2D\n",
    "from keras import callbacks\n",
    "from keras import metrics as keras_metrics\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras import regularizers\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred): \n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.lr = []\n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.lr.append(step_decay(len(self.losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simply_blend(prediction_dataframes, weights, target_column_name='target'):\n",
    "    blended_prediction_df = pd.DataFrame(data=np.zeros(prediction_dataframes[0].shape[0]), columns=[target_column_name])\n",
    "    for prediction_df, weight in zip(prediction_dataframes, weights):\n",
    "        blended_prediction_df[target_column_name] = blended_prediction_df[target_column_name] + weight * prediction_df[target_column_name]\n",
    "        #blended_prediction_df.add(weight * prediction_df)\n",
    "    return blended_prediction_df / sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_keras_nn(train_features, train_targets, train_meta_df, train_meta_column_name, model, batch_size=100, epochs=20, n_splits=5):\n",
    "    loss_history = LossHistory()\n",
    "    lrate = LearningRateScheduler(step_decay)\n",
    "    callbacks_list = [EarlyStopping(monitor='val_auc', patience=20, mode='max'), loss_history, annealer]\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits)\n",
    "    start_time = time.time()\n",
    "    print(type(train_features))\n",
    "    for train_index, val_index in sss.split(train_features, train_targets):\n",
    "        #print(type(train_index))\n",
    "        #print(train_index)\n",
    "        X_train = train_features[train_index]\n",
    "        X_val = train_features[val_index]\n",
    "        Y_train = train_targets[train_index]\n",
    "        Y_val = train_targets[val_index]\n",
    "        #X_tr, Y_tr = augment(X_train, Y_train)\n",
    "        #print(\"{} iteration\".format(i+1))\n",
    "        history= model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, callbacks=callbacks_list, verbose=1, validation_data=(X_val,Y_val))\n",
    "        predictions = model.predict(X_val)[:, 0]\n",
    "        print(\"val_index.shape: \", val_index.shape)\n",
    "        print(\"predictions.shape: \", predictions.shape)\n",
    "        train_meta_df[train_meta_column_name].iloc[val_index] = predictions\n",
    "        #history= sequential_nn_model.fit(X_train, Y_train, batch_size=batch_size, epochs=50, callbacks=callbacks_list, verbose=1, validation_data=(X_val,Y_val))\n",
    "        del X_train, X_val, Y_train, Y_val\n",
    "        gc.collect()\n",
    "    print(\"Run time {} min\".format((time.time() - start_time) / 60))\n",
    "    return model, train_meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(train_df, test_df, target, features, param, num_round=1000000):\n",
    "    start_time = time.time()\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=False, random_state=44000)\n",
    "    oof = np.zeros(len(train_df))\n",
    "    predictions = np.zeros(len(test_df))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    lgb_classifier = None\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n",
    "        print(\"Fold {}\".format(fold_))\n",
    "        trn_data = lgb.Dataset(train_df.iloc[trn_idx][features], label=target.iloc[trn_idx])\n",
    "        val_data = lgb.Dataset(train_df.iloc[val_idx][features], label=target.iloc[val_idx])\n",
    "        num_round = num_round\n",
    "        clf = lgb.train(\n",
    "            param,\n",
    "            trn_data,\n",
    "            num_round,\n",
    "            valid_sets=[trn_data, val_data],\n",
    "            verbose_eval=1000,\n",
    "            early_stopping_rounds=3000\n",
    "        )\n",
    "        lgb_classifier = clf\n",
    "        oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['Feature'] = features\n",
    "        fold_importance_df['importance'] = clf.feature_importance()\n",
    "        fold_importance_df['fold'] = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        predictions += clf.predict(test_df[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "    print(\"Total run time {} min:\".format((time.time() - start_time) / 60))\n",
    "    print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))\n",
    "    return oof, predictions, feature_importance_df, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pdf_difference(feat, df_feature, df_target, IQR_multiplier, bin_bandwidth_multiplier, print_number_bins):\n",
    "    #Agreggating feature values in bin format using the Freedman-Diaconis rule\n",
    "    IQR = df_feature[feat].quantile([0.75]).values - df_feature[feat].quantile([0.25]).values #Interquartile range (IQR)\n",
    "    n = len(df_feature[feat])\n",
    "    bin_size = IQR_multiplier*IQR/n**(1/3)\n",
    "    bin_number = int(np.round((df_feature[feat].max() - df_feature[feat].min())/bin_size))\n",
    "    binvalues = pd.cut(df_feature[feat], bins = bin_number, labels = range(bin_number)).astype('float')\n",
    "    \n",
    "    if print_number_bins:\n",
    "        print('There are {} bins in the feature {}'.format(bin_number, feat))\n",
    "\n",
    "    #Calculate the PDFs using the df_target\n",
    "    pdf_0 = KernelDensity(kernel='gaussian', bandwidth=bin_size*bin_bandwidth_multiplier)\n",
    "    pdf_0.fit(np.array(df_target[feat][df_target['target'] == 0]).reshape(-1,1))\n",
    "\n",
    "    pdf_1 = KernelDensity(kernel='gaussian', bandwidth=bin_size*bin_bandwidth_multiplier)\n",
    "    pdf_1.fit(np.array(df_target[feat][df_target['target'] == 1]).reshape(-1,1))\n",
    "\n",
    "    #Creates an X array with the average feature value for each bin\n",
    "    x = np.array(np.arange(min(df_feature[feat]) + bin_size/2 ,max(df_feature[feat]), bin_size)).reshape(-1,1)\n",
    "\n",
    "    #gets the pdf values based on the X array\n",
    "    log_pdf_0 = np.exp(pdf_0.score_samples(x))\n",
    "    log_pdf_1 = np.exp(pdf_1.score_samples(x))\n",
    "\n",
    "    #creates a dictionary that links the bin number with the PDFs value difference\n",
    "    pdf_dict = dict()\n",
    "    for i in range(bin_number):\n",
    "        pdf_dict[i] = log_pdf_1[i] - log_pdf_0[i] \n",
    "\n",
    "    #gets the PDF difference for each row of the dataset based on its equivalent bin.\n",
    "    bin_pdf_values = np.array(itemgetter(*list(binvalues))(pdf_dict))\n",
    "\n",
    "    return bin_pdf_values, x, log_pdf_0, log_pdf_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = [c for c in train_df.columns if c not in ['ID_code', 'target']]\n",
    "train_features_df = train_df[train_df.columns.drop(['ID_code', 'target'])].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_series = train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id_codes = train_df['ID_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values, holdout_test_values, train_target_values, holdout_test_target_values = train_test_split(\n",
    "    train_features_df.values,\n",
    "    target_series.values,\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, holdout_test_df, train_target_series, holdout_test_target_series = train_test_split(\n",
    "    train_features_df,\n",
    "    target_series,\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train_values.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_regularizer=regularizers.l2(0.01)\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(input_dim,)))\n",
    "#model.add(PreLU(alpha=.001))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64, input_shape=(input_dim / 2, ), activation='relu'))\n",
    "#model.add(PreLU(alpha=.001))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32, input_shape=(input_dim / 4, ), activation='relu'))\n",
    "#model.add(PreLU(alpha=.001))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "annealer = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_nn_model = Sequential()\n",
    "sequential_nn_model.add(Dense(batch_size, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "sequential_nn_model.add(Dropout(0.1))\n",
    "#sequential_nn_model.add(Dropout(0.4))\n",
    "sequential_nn_model.add(BatchNormalization())\n",
    "sequential_nn_model.add(Dense(batch_size, input_dim=input_dim / 2, kernel_initializer='normal', activation='sigmoid'))\n",
    "####sequential_nn_model.add(Dense(batch_size, input_dim=input_dim / 10, kernel_initializer='normal', activation='sigmoid'))\n",
    "sequential_nn_model.add(Dropout(0.1))\n",
    "####sequential_nn_model.add(Dropout(0.4))\n",
    "sequential_nn_model.add(BatchNormalization())\n",
    "sequential_nn_model.add(Dense(batch_size, input_dim=input_dim / 4, kernel_initializer='normal', activation='relu'))\n",
    "sequential_nn_model.add(Dropout(0.1))\n",
    "sequential_nn_model.add(BatchNormalization())\n",
    "sequential_nn_model.add(Dense(batch_size, input_dim=input_dim / 4, kernel_initializer='normal', activation='sigmoid'))\n",
    "#sequential_nn_model.add(Dense(batch_size, input_shape=(100, 200), kernel_initializer='normal', activation='sigmoid'))\n",
    "#sequential_nn_model.add(Dropout(0.76))\n",
    "#sequential_nn_model.add(Dropout(0.24))\n",
    "sequential_nn_model.add(Dropout(0.1))\n",
    "sequential_nn_model.add(BatchNormalization())\n",
    "sequential_nn_model.add(Dense(batch_size, input_dim=input_dim / 30, kernel_initializer='normal', activation='relu'))\n",
    "sequential_nn_model.add(Dropout(0.1))\n",
    "sequential_nn_model.add(BatchNormalization())\n",
    "sequential_nn_model.add(Dense(batch_size, input_dim=input_dim / 30, kernel_initializer='normal', activation='sigmoid'))\n",
    "sequential_nn_model.add(Dropout(0.1))\n",
    "sequential_nn_model.add(BatchNormalization())\n",
    "sequential_nn_model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequential_model_min(input_dim):\n",
    "    sequential_nn_model_min = Sequential()\n",
    "    sequential_nn_model_min.add(Dense(batch_size, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "    sequential_nn_model_min.add(Dropout(0.4))\n",
    "    sequential_nn_model_min.add(BatchNormalization())\n",
    "    sequential_nn_model_min.add(Dense(batch_size, input_dim=input_dim / 10, kernel_initializer='normal', activation='sigmoid'))\n",
    "    sequential_nn_model_min.add(Dropout(0.4))\n",
    "    sequential_nn_model_min.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    return sequential_nn_model_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_nn_model_min = make_sequential_model_min(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_nn_model_min.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_df = train_features_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_df['nn_meta'] = np.NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_df['bayes_meta'] = np.NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_meta_df['nn_meta'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction = pd.Series([0.2, 0.22, 0.3, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_meta_df['nn_meta'].iloc[[2, 3, 4, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_meta_df['nn_meta'].iloc[[2, 3, 4, 5]] = prediction.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_meta_df['nn_meta'].iloc[[2, 3, 4, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_meta_df['nn_meta'].iloc[[0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_values, holdout_test_values, train_target_values, holdout_test_target_values\n",
    "train_nn_result = train_keras_nn(train_values, train_target_values, train_meta_df, 'nn_meta', sequential_nn_model_min, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_nn_model_min, train_meta_df = train_nn_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_df['nn_meta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques_dict = {column_name: train_df[column_name].unique() for column_name in train_df.columns.drop(['ID_code','target']).tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques_dict_counts = {column_name: uniques.shape[0] for column_name, uniques in uniques_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques_counts_series = pd.Series(uniques_dict_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uniques_counts_series.unique().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uniques_counts_series.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uniques_counts_series.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_param = {\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.4,\n",
    "    'boost_from_average':'false',\n",
    "    'boost': 'gbdt',\n",
    "    'feature_fraction': 0.05,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': -1,  \n",
    "    'metric':'auc',\n",
    "    'min_data_in_leaf': 80,\n",
    "    'min_sum_hessian_in_leaf': 10.0,\n",
    "    'num_leaves': 13,\n",
    "    'num_threads': 8,\n",
    "    'tree_learner': 'serial',\n",
    "    'objective': 'binary', \n",
    "    'verbosity': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n",
    "        .groupby(\"Feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:150].index)\n",
    "best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,28))\n",
    "sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n",
    "plt.title('Features importance (averaged/folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('FI.png')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df_rows_count = train_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df_rows_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniques_count_more_1_2 = uniques_counts_series[uniques_counts_series > train_df_rows_count / 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniques_count_more_1_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniques_count_more_1_4_less_1_2 = uniques_counts_series[uniques_counts_series < train_df_rows_count / 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniques_count_less_1_2_more_1_4 = uniques_count_more_1_4_less_1_2[uniques_count_more_1_4_less_1_2 > train_df_rows_count / 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniques_count_more_1_4_less_1_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniques_count_less_1_4 = uniques_counts_series[uniques_counts_series < train_df_rows_count / 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniques_count_less_1_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_whole = train_lgbm(\n",
    "    train_values,\n",
    "    test_df,\n",
    "    train_target_values,\n",
    "    train_df.columns.drop(['ID_code', 'target']).tolist(),\n",
    "    lgbm_param\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oof_ucm_whole, predictions_whole, feature_importance_whole, clf_ucm_whole = train_results_whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions_df_whole = pd.DataFrame(data=predictions_whole, columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission_whole_only_df = pd.DataFrame({'ID_code': ID_code, 'target': predictions_df_whole['target'].values.astype('float32')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission_whole_only_df.to_csv('submission_whole_only.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#secquential_nn_model_min = train_keras_nn(train_polinomial_values_ucm_1_2, train_target_values_ucm_1_2, sequential_nn_model_min_1_2, batch_size=512, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_and_metrics = sequential_nn_model_min.evaluate(holdout_test_polinomial_values_ucm_1_2, holdout_test_target_values_ucm_1_2, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequential_nn_model_min.save('secquential_nn_model_min.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id_code = test_df['ID_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_submission_df = pd.read_csv('submission_mlp_not_my.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_submission_df = pd.read_csv('submission_whole_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simply_blend_gb_and_nn_df = simply_blend([mlp_submission_df, whole_submission_df], [1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\"ID_code\" : test_id_code.values, \"target\" : simply_blend_gb_and_nn_df['target']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('simply_blend_gb_and_nn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_df['log_reg_meta'] = np.NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model = LogisticRegression(C=1, n_jobs=10, penalty=\"l2\", solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df\n",
    "target = train_target_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "start_time = time.time()\n",
    "aucs=[]\n",
    "\n",
    "test_preds = []\n",
    "# for early stopping\n",
    "# it takes a long time if using all the samples.\n",
    "samples = train.shape[0]\n",
    "for fold,(train_idx, val_idx) in enumerate(kfold.split(train, target)):\n",
    "    print(\"####################################\")\n",
    "    print(\"############fold:\", fold)\n",
    "    sample_x = train.iloc[train_idx].values\n",
    "    sample_y = target.iloc[train_idx].values\n",
    "    \n",
    "    sample_val_x = train.iloc[val_idx].values\n",
    "    sample_val_y = target.iloc[val_idx].values\n",
    "    \n",
    "    log_reg_model.fit(sample_x,sample_y)\n",
    "    y_pred_prob = log_reg_model.predict_proba(sample_x)[:,1]\n",
    "    #y_val_pred_prob = model.predict_proba(sample_val_x)[:,1]\n",
    "    y_val_pred_prob = model.predict_proba(sample_val_x)[:, 0]\n",
    "    print(\"type(val_idx): \", type(val_idx))\n",
    "    print(\"type(y_val_pred_proba): \", type(y_val_pred_prob))\n",
    "    print(\"val_idx.shape: \", val_idx.shape)\n",
    "    print(\"y_val_pred_proba.shape: \", y_val_pred_prob.shape)\n",
    "    print(\"val_idx: \", val_idx)\n",
    "    print(\"y_val_pred_proba: \", y_val_pred_prob)    \n",
    "    train_meta_df['log_reg_meta'].iloc[val_idx] = y_val_pred_prob\n",
    "   \n",
    "    train_auc = metrics.roc_auc_score(sample_y,y_pred_prob)\n",
    "    val_auc = metrics.roc_auc_score(sample_val_y,y_val_pred_prob)\n",
    "    print(\"train auc:{},val auc:{}\".format(train_auc,val_auc))\n",
    "    aucs.append([train_auc,val_auc])\n",
    "    test_preds.append(model.predict_proba(test)[:,1])\n",
    "    \n",
    "end_time=time.time()\n",
    "val_aucs=[auc[1] for auc in aucs]\n",
    "print(\"using {} samples,total time:{}s,mean val auc:{}\".format(samples,end_time-start_time,np.mean(val_aucs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat1, feat2 = 'var_81', 'var_139'\n",
    "\n",
    "fig = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "#plot pdf feat 1\n",
    "bin_pdf_values, x, log_pdf_0, log_pdf_1 = calculate_pdf_difference(feat = feat1, df_feature = full, df_target = train_df, IQR_multiplier = 2, bin_bandwidth_multiplier = 1.5, print_number_bins = True)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "sns.kdeplot(train_df[feat1][train_df['target'] == 0], shade=False, color=\"b\", label = 'target = 0')\n",
    "sns.kdeplot(train_df[feat1][train_df['target'] == 1], shade=False, color=\"r\", label = 'target = 1')\n",
    "plt.plot(x, log_pdf_0)\n",
    "plt.plot(x, log_pdf_1) \n",
    "plt.title(feat1)\n",
    "plt.xlabel('Feature Values')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "#plot pdf feat 2\n",
    "bin_pdf_values, x, log_pdf_0, log_pdf_1 = calculate_pdf_difference(feat = feat2, df_feature = full, df_target = train_df, IQR_multiplier = 2, bin_bandwidth_multiplier = 1.5, print_number_bins = True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.kdeplot(train_df[feat2][train_df['target'] == 0], shade=False, color=\"b\", label = 'target = 0')\n",
    "sns.kdeplot(train_df[feat2][train_df['target'] == 1], shade=False, color=\"r\", label = 'target = 1')\n",
    "plt.plot(x, log_pdf_0)\n",
    "plt.plot(x, log_pdf_1) \n",
    "plt.title(feat2)\n",
    "plt.xlabel('Feature Values')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
